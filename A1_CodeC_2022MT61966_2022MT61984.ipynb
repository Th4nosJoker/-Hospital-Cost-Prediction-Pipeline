{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# =============================================================================\n",
        "# Custom transformers\n",
        "# =============================================================================\n",
        "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Convert an array to a pandas DataFrame. \"\"\"\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X[self.columns]\n",
        "        return pd.DataFrame(X, columns=self.columns)\n",
        "\n",
        "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Convert sparse matrix to dense. \"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.toarray() if sp.issparse(X) else X\n",
        "\n",
        "# =============================================================================\n",
        "# Load Data\n",
        "# =============================================================================\n",
        "train_df = pd.read_csv('kaggle_train.csv')\n",
        "test_df = pd.read_csv('kaggle_test.csv')\n",
        "\n",
        "# =============================================================================\n",
        "# Define Feature Lists\n",
        "# =============================================================================\n",
        "categorical_features = [\n",
        "    'Hospital Service Area', 'Hospital County', 'Operating Certificate Number',\n",
        "    'Permanent Facility Id', 'Facility Name', 'Age Group', 'Zip Code - 3 digits',\n",
        "    'Race', 'Ethnicity', 'Type of Admission', 'Patient Disposition',\n",
        "    'CCSR Diagnosis Code', 'CCSR Procedure Code', 'APR DRG Code',\n",
        "    'APR MDC Code', 'APR Severity of Illness Description',\n",
        "    'APR Risk of Mortality', 'APR Medical Surgical Description',\n",
        "    'Payment Typology 1', 'Payment Typology 2', 'Payment Typology 3',\n",
        "    'Emergency Department Indicator'\n",
        "]\n",
        "numerical_features = ['Length of Stay', 'Birth Weight']\n",
        "\n",
        "# =============================================================================\n",
        "# Build Preprocessing Pipelines\n",
        "# =============================================================================\n",
        "# Numeric pipeline: impute missing values and scale.\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: impute, encode, and convert.\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
        "    ('to_df', ToDataFrame(columns=categorical_features)),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
        "])\n",
        "\n",
        "# Combine both pipelines.\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numerical_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "], remainder='drop')\n",
        "\n",
        "# =============================================================================\n",
        "# Split Data\n",
        "# =============================================================================\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "X_train = train_data.drop('Total Costs', axis=1)\n",
        "y_train = train_data['Total Costs']\n",
        "X_val = val_data.drop('Total Costs', axis=1)\n",
        "y_val = val_data['Total Costs']\n",
        "X_test = test_df  # Test set (without target)\n",
        "\n",
        "# =============================================================================\n",
        "# Preprocess Data\n",
        "# =============================================================================\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_val_preprocessed = preprocessor.transform(X_val)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Convert sparse matrices if needed\n",
        "if sp.issparse(X_train_preprocessed):\n",
        "    X_train_preprocessed = X_train_preprocessed.tocsr()\n",
        "    X_val_preprocessed = X_val_preprocessed.tocsr()\n",
        "    X_test_preprocessed = X_test_preprocessed.tocsr()\n",
        "\n",
        "# =============================================================================\n",
        "# Train XGBoost Model\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "num_round = 1000  # Number of boosting rounds\n",
        "\n",
        "\n",
        "# Set up XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 8,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 45,\n",
        "}\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain = xgb.DMatrix(X_train_preprocessed, label=y_train)\n",
        "dval = xgb.DMatrix(X_val_preprocessed, label=y_val)\n",
        "\n",
        "# Train model\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,  # Correct way to set number of iterations\n",
        "    evals=[(dtrain, 'train'), (dval, 'eval')],\n",
        "    early_stopping_rounds=10,  # Pass as a function argument, not in params\n",
        "    verbose_eval=100\n",
        ")\n",
        "\n",
        "# Predict and evaluate\n",
        "preds = model.predict(dval)\n",
        "mse = mean_squared_error(y_val, preds)\n",
        "print(f\"Validation MSE: {mse:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model.save_model('xgboost_model.json')"
      ],
      "metadata": {
        "id": "7J8NDL-8yCah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd23e20-b69b-4392-9e11-dda6f8e9444f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-rmse:52712.26430\teval-rmse:60991.19501\n",
            "[100]\ttrain-rmse:14191.68110\teval-rmse:28013.17287\n",
            "[200]\ttrain-rmse:12010.14304\teval-rmse:27202.56032\n",
            "[300]\ttrain-rmse:10951.45154\teval-rmse:26872.48656\n",
            "[369]\ttrain-rmse:10408.31722\teval-rmse:26799.44309\n",
            "Validation MSE: 718109966.8685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtest = xgb.DMatrix(data=X_test_preprocessed)\n",
        "# Generate predictions on the test set.\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "print(\"Test Predictions from XGBoost (trained model):\")\n",
        "print(test_predictions)\n",
        "\n",
        "# 'test_predictions' is assumed to be the array of predictions obtained from your XGBoost model.\n",
        "# Create a DataFrame that includes both the ID column and the predictions.\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_df['ID'],            # Use the ID column from the original test file\n",
        "    'Total Costs': test_predictions # Predictions from the model\n",
        "})\n",
        "\n",
        "# Display the first few rows of the submission DataFrame\n",
        "print(submission.head())\n",
        "\n",
        "# Optionally, save the submission DataFrame to a CSV file:\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D_KOLQVHR1o",
        "outputId": "fa81517c-7510-4389-c6b9-46ba01e54644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Predictions from XGBoost (trained model):\n",
            "[26442.68   17785.828  17512.256  ... 20956.748  70944.266   2601.7563]\n",
            "   ID   Total Costs\n",
            "0   1  26442.679688\n",
            "1   2  17785.828125\n",
            "2   3  17512.255859\n",
            "3   4  50592.515625\n",
            "4   5  42943.617188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TT6qaRFILJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}